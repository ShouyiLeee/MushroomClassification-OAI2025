{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c138ffb",
   "metadata": {},
   "source": [
    "# Sys check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ad0358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:21:33.445626Z",
     "iopub.status.busy": "2025-04-15T15:21:33.445049Z",
     "iopub.status.idle": "2025-04-15T15:21:33.454441Z",
     "shell.execute_reply": "2025-04-15T15:21:33.453683Z",
     "shell.execute_reply.started": "2025-04-15T15:21:33.445605Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root = os.getcwd()\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd2949b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:21:33.455933Z",
     "iopub.status.busy": "2025-04-15T15:21:33.455755Z",
     "iopub.status.idle": "2025-04-15T15:21:33.664645Z",
     "shell.execute_reply": "2025-04-15T15:21:33.663752Z",
     "shell.execute_reply.started": "2025-04-15T15:21:33.455919Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 15 15:21:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0             27W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4a41a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:21:33.665951Z",
     "iopub.status.busy": "2025-04-15T15:21:33.665731Z",
     "iopub.status.idle": "2025-04-15T15:21:37.744014Z",
     "shell.execute_reply": "2025-04-15T15:21:37.743312Z",
     "shell.execute_reply.started": "2025-04-15T15:21:33.665930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df7dfc",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be37c04b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:21:37.745033Z",
     "iopub.status.busy": "2025-04-15T15:21:37.744678Z",
     "iopub.status.idle": "2025-04-15T15:21:41.441138Z",
     "shell.execute_reply": "2025-04-15T15:21:41.440608Z",
     "shell.execute_reply.started": "2025-04-15T15:21:37.745011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb4e4719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:21:41.443004Z",
     "iopub.status.busy": "2025-04-15T15:21:41.442674Z",
     "iopub.status.idle": "2025-04-15T15:21:41.448291Z",
     "shell.execute_reply": "2025-04-15T15:21:41.447599Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.442986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def training(model, optimizer, criterion, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {running_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        print(\"Validation performance:\")\n",
    "        evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "26415dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:47:09.479047Z",
     "iopub.status.busy": "2025-04-15T16:47:09.478341Z",
     "iopub.status.idle": "2025-04-15T16:47:09.486235Z",
     "shell.execute_reply": "2025-04-15T16:47:09.485407Z",
     "shell.execute_reply.started": "2025-04-15T16:47:09.479015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),  \n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, \n",
    "                            translate=(0.1, 0.1)),  # (6) Dịch vị trí\n",
    "    transforms.ColorJitter(brightness=0.1),     # (5) Điều chỉnh độ sáng \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean/std\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cd2b454c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:10:42.704224Z",
     "iopub.status.busy": "2025-04-15T17:10:42.703977Z",
     "iopub.status.idle": "2025-04-15T17:10:42.708207Z",
     "shell.execute_reply": "2025-04-15T17:10:42.707406Z",
     "shell.execute_reply.started": "2025-04-15T17:10:42.704204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 4\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-5\n",
    "num_images_per_sample=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ef3732",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T15:21:41.485727Z",
     "iopub.status.busy": "2025-04-15T15:21:41.485530Z",
     "iopub.status.idle": "2025-04-15T15:21:41.500818Z",
     "shell.execute_reply": "2025-04-15T15:21:41.500142Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.485707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hàm đánh giá\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"bào ngư xám + trắng\", \"Đùi gà Baby (cắt ngắn)\", \"nấm mỡ\", \"linh chi trắng\"])\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69183753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:12:15.021521Z",
     "iopub.status.busy": "2025-04-15T16:12:15.021030Z",
     "iopub.status.idle": "2025-04-15T16:12:15.032008Z",
     "shell.execute_reply": "2025-04-15T16:12:15.031230Z",
     "shell.execute_reply.started": "2025-04-15T16:12:15.021498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class MultiImageMushroomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, num_images_per_sample=4):\n",
    "        \"\"\"\n",
    "        Dataset phân loại nấm sử dụng nhiều ảnh một mẫu.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Thư mục gốc chứa các class folders.\n",
    "            transform (callable, optional): Transform áp dụng lên ảnh.\n",
    "            num_images_per_sample (int): Số ảnh muốn nhóm lại thành 1 sample (mặc định 4).\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.num_images_per_sample = num_images_per_sample\n",
    "\n",
    "        # Map từ prefix sang class label\n",
    "        self.prefix2class = {\n",
    "            'NM': 'nấm mỡ',\n",
    "            'BN': 'bào ngư xám + trắng',\n",
    "            'DG': 'Đùi gà Baby (cắt ngắn)',\n",
    "            'LC': 'linh chi trắng'\n",
    "        }\n",
    "        self.class_names = list(self.prefix2class.values())\n",
    "        self.class2idx = {name: idx for idx, name in enumerate(self.class_names)}\n",
    "\n",
    "        # Duyệt từng class folder\n",
    "        for class_name in self.class_names:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            images = sorted(glob.glob(os.path.join(class_path, \"*.jpg\")))\n",
    "\n",
    "            # Gom ảnh theo prefix (BM, AB, ...)\n",
    "            prefix_groups = {}\n",
    "            for img_path in images:\n",
    "                filename = os.path.basename(img_path)\n",
    "                match = re.match(r\"([A-Z]{2})\\d+\", filename)\n",
    "                if match:\n",
    "                    prefix = match.group(1)\n",
    "                    if prefix not in prefix_groups:\n",
    "                        prefix_groups[prefix] = []\n",
    "                    prefix_groups[prefix].append(img_path)\n",
    "\n",
    "            # Tạo samples từ các nhóm ảnh\n",
    "            for prefix, img_list in prefix_groups.items():\n",
    "                img_list = sorted(img_list)\n",
    "                label_idx = self.class2idx[self.prefix2class[prefix]]\n",
    "\n",
    "                for i in range(0, len(img_list), self.num_images_per_sample):\n",
    "                    selected = img_list[i:i+self.num_images_per_sample]\n",
    "                    if len(selected) < self.num_images_per_sample:\n",
    "                        selected = (selected + [selected[0]] * self.num_images_per_sample)[:self.num_images_per_sample]\n",
    "                    self.samples.append((selected, label_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_paths, label = self.samples[idx]\n",
    "        imgs = []\n",
    "\n",
    "        for path in img_paths:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            imgs.append(image)\n",
    "\n",
    "        return torch.stack(imgs), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "946e2f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:12:20.293422Z",
     "iopub.status.busy": "2025-04-15T16:12:20.293131Z",
     "iopub.status.idle": "2025-04-15T16:12:20.300797Z",
     "shell.execute_reply": "2025-04-15T16:12:20.300061Z",
     "shell.execute_reply.started": "2025-04-15T16:12:20.293400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm  # For progress bars in notebooks\n",
    "\n",
    "def create_dataloaders(dataset_root, transform, num_images_per_sample=2, \n",
    "                      batch_size=16, val_split=0.05, seed=42, \n",
    "                      num_workers=0, pin_memory=True):\n",
    "    \"\"\"\n",
    "    Create optimized train and validation dataloaders with better error handling\n",
    "    and performance settings.\n",
    "    \n",
    "    Args:\n",
    "        dataset_root (str): Root directory for dataset\n",
    "        transform: Data transformations to apply\n",
    "        num_images_per_sample (int): Number of images per sample\n",
    "        batch_size (int): Batch size for training\n",
    "        val_split (float): Validation split ratio (0-1)\n",
    "        seed (int): Random seed for reproducibility\n",
    "        num_workers (int): Number of workers for data loading (0 for no multiprocessing)\n",
    "        pin_memory (bool): Whether to pin memory for faster GPU transfer\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader)\n",
    "    \"\"\"\n",
    "    # Check if dataset directory exists\n",
    "    if not os.path.exists(dataset_root):\n",
    "        raise FileNotFoundError(f\"Dataset directory not found: {dataset_root}\")\n",
    "    \n",
    "    print(f\"Loading dataset from {dataset_root}...\")\n",
    "    \n",
    "    # Create dataset with progress reporting\n",
    "    try:\n",
    "        dataset = MultiImageMushroomDataset(\n",
    "            root_dir=dataset_root,\n",
    "            transform=transform,\n",
    "            num_images_per_sample=num_images_per_sample\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataset: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "    \n",
    "    # Set random seed for reproducible splits\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "    \n",
    "    print(f\"Split: {train_size} training samples, {val_size} validation samples\")\n",
    "    \n",
    "    # Create dataloaders with optimized settings\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,  # Set to 0 to avoid pickle errors\n",
    "        pin_memory=pin_memory,    # Faster data transfer to GPU\n",
    "        drop_last=False,          # Use all samples\n",
    "        persistent_workers=(num_workers > 0),  # Keep workers alive between epochs\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,  # No need to shuffle validation data\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "52cf9557-6574-4a0d-b821-af25db093e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:03:26.001357Z",
     "iopub.status.busy": "2025-04-15T17:03:26.000784Z",
     "iopub.status.idle": "2025-04-15T17:03:26.013603Z",
     "shell.execute_reply": "2025-04-15T17:03:26.013013Z",
     "shell.execute_reply.started": "2025-04-15T17:03:26.001332Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /kaggle/input/aio-hutech/train...\n",
      "Dataset loaded: 300 samples\n",
      "Split: 285 training samples, 15 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders with optimized settings\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    dataset_root=\"/kaggle/input/aio-hutech/train\",\n",
    "    transform=transform,\n",
    "    num_images_per_sample=num_images_per_sample,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,  # Fix pickle error by using 0 workers\n",
    "    pin_memory=torch.cuda.is_available(),  # Only pin if CUDA is available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2680d60f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:03:28.467725Z",
     "iopub.status.busy": "2025-04-15T17:03:28.467014Z",
     "iopub.status.idle": "2025-04-15T17:03:28.472228Z",
     "shell.execute_reply": "2025-04-15T17:03:28.471622Z",
     "shell.execute_reply.started": "2025-04-15T17:03:28.467703Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d0bb8bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:03:32.112713Z",
     "iopub.status.busy": "2025-04-15T17:03:32.112416Z",
     "iopub.status.idle": "2025-04-15T17:03:32.256193Z",
     "shell.execute_reply": "2025-04-15T17:03:32.255370Z",
     "shell.execute_reply.started": "2025-04-15T17:03:32.112694Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label batch: tensor([1, 1, 1, 0, 3, 2, 3, 1])\n",
      "Max label: tensor(3)\n",
      "Min label: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in val_loader:\n",
    "    print(\"Label batch:\", labels)\n",
    "    print(\"Max label:\", labels.max())\n",
    "    print(\"Min label:\", labels.min())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "42ad3675",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:03:34.323395Z",
     "iopub.status.busy": "2025-04-15T17:03:34.323155Z",
     "iopub.status.idle": "2025-04-15T17:03:34.329446Z",
     "shell.execute_reply": "2025-04-15T17:03:34.328718Z",
     "shell.execute_reply.started": "2025-04-15T17:03:34.323380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViT_MushroomClassifier(nn.Module):\n",
    "    def __init__(self, vit_model_name='vit_base_patch16_224', num_classes=4):\n",
    "        super(ViT_MushroomClassifier, self).__init__()\n",
    "        self.vit = timm.create_model(vit_model_name, pretrained=True)\n",
    "        self.vit.head = nn.Identity()  # Bỏ classification head của ViT\n",
    "\n",
    "        self.embedding_dim = self.vit.num_features  # Thường là 768\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 4, C, H, W]\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B * N, C, H, W)\n",
    "\n",
    "        embeddings = self.vit(x)  # [B*4, D]\n",
    "        embeddings = embeddings.view(B, N, -1)  # [B, 4, D]\n",
    "\n",
    "        # Mean pooling over 4 embeddings\n",
    "        pooled = embeddings.mean(dim=1)  # [B, D]\n",
    "        out = self.classifier(pooled)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11172ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:10:50.059251Z",
     "iopub.status.busy": "2025-04-15T17:10:50.058795Z",
     "iopub.status.idle": "2025-04-15T17:10:51.693890Z",
     "shell.execute_reply": "2025-04-15T17:10:51.693316Z",
     "shell.execute_reply.started": "2025-04-15T17:10:50.059232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = ViT_MushroomClassifier(vit_model_name='vit_base_patch16_224', num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0d9e550a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:10:51.695355Z",
     "iopub.status.busy": "2025-04-15T17:10:51.695041Z",
     "iopub.status.idle": "2025-04-15T17:10:51.700018Z",
     "shell.execute_reply": "2025-04-15T17:10:51.699342Z",
     "shell.execute_reply.started": "2025-04-15T17:10:51.695332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# loss và optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0e620afe",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-15T17:10:51.780559Z",
     "iopub.status.busy": "2025-04-15T17:10:51.780336Z",
     "iopub.status.idle": "2025-04-15T17:15:02.576992Z",
     "shell.execute_reply": "2025-04-15T17:15:02.576221Z",
     "shell.execute_reply.started": "2025-04-15T17:10:51.780544Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Loss: 0.9397\n",
      "Validation performance:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       0.80      1.00      0.89         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      0.50      0.67         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           0.93        15\n",
      "             macro avg       0.95      0.88      0.89        15\n",
      "          weighted avg       0.95      0.93      0.93        15\n",
      "\n",
      "[Epoch 2/10] Loss: 0.1488\n",
      "Validation performance:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       1.00      1.00      1.00         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      1.00      1.00         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           1.00        15\n",
      "             macro avg       1.00      1.00      1.00        15\n",
      "          weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[Epoch 3/10] Loss: 0.0195\n",
      "Validation performance:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       1.00      1.00      1.00         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      1.00      1.00         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           1.00        15\n",
      "             macro avg       1.00      1.00      1.00        15\n",
      "          weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[Epoch 4/10] Loss: 0.0104\n",
      "Validation performance:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       1.00      1.00      1.00         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      1.00      1.00         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           1.00        15\n",
      "             macro avg       1.00      1.00      1.00        15\n",
      "          weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[Epoch 5/10] Loss: 0.0052\n",
      "Validation performance:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       1.00      1.00      1.00         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      1.00      1.00         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           1.00        15\n",
      "             macro avg       1.00      1.00      1.00        15\n",
      "          weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[Epoch 6/10] Loss: 0.0032\n",
      "Validation performance:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       1.00      1.00      1.00         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      1.00      1.00         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           1.00        15\n",
      "             macro avg       1.00      1.00      1.00        15\n",
      "          weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[Epoch 7/10] Loss: 0.0025\n",
      "Validation performance:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       1.00      1.00      1.00         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      1.00      1.00         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           1.00        15\n",
      "             macro avg       1.00      1.00      1.00        15\n",
      "          weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[Epoch 8/10] Loss: 0.0017\n",
      "Validation performance:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       1.00      1.00      1.00         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      1.00      1.00         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           1.00        15\n",
      "             macro avg       1.00      1.00      1.00        15\n",
      "          weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[Epoch 9/10] Loss: 0.0016\n",
      "Validation performance:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       1.00      1.00      1.00         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      1.00      1.00         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           1.00        15\n",
      "             macro avg       1.00      1.00      1.00        15\n",
      "          weighted avg       1.00      1.00      1.00        15\n",
      "\n",
      "[Epoch 10/10] Loss: 0.0015\n",
      "Validation performance:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       1.00      1.00      1.00         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      1.00      1.00         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           1.00        15\n",
      "             macro avg       1.00      1.00      1.00        15\n",
      "          weighted avg       1.00      1.00      1.00        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training(model, optimizer, criterion, train_loader, val_loader, num_epochs=num_epochs, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8952ba15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:15:02.578742Z",
     "iopub.status.busy": "2025-04-15T17:15:02.578518Z",
     "iopub.status.idle": "2025-04-15T17:15:03.132299Z",
     "shell.execute_reply": "2025-04-15T17:15:03.131691Z",
     "shell.execute_reply.started": "2025-04-15T17:15:02.578725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "   bào ngư xám + trắng       1.00      1.00      1.00         4\n",
      "Đùi gà Baby (cắt ngắn)       1.00      1.00      1.00         6\n",
      "                nấm mỡ       1.00      1.00      1.00         2\n",
      "        linh chi trắng       1.00      1.00      1.00         3\n",
      "\n",
      "              accuracy                           1.00        15\n",
      "             macro avg       1.00      1.00      1.00        15\n",
      "          weighted avg       1.00      1.00      1.00        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "89d8a4da-945e-49ac-b45d-b5e0590d60df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:23:17.214110Z",
     "iopub.status.busy": "2025-04-15T17:23:17.213861Z",
     "iopub.status.idle": "2025-04-15T17:23:17.651019Z",
     "shell.execute_reply": "2025-04-15T17:23:17.650402Z",
     "shell.execute_reply.started": "2025-04-15T17:23:17.214094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"vit_mushroom_multi_4_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2c6cc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T15:21:41.807152Z",
     "iopub.status.idle": "2025-04-15T15:21:41.807430Z",
     "shell.execute_reply": "2025-04-15T15:21:41.807286Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.807274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"vit_mushroom_multi_3_best.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d440703f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:05:48.054940Z",
     "iopub.status.busy": "2025-04-15T17:05:48.054615Z",
     "iopub.status.idle": "2025-04-15T17:05:48.060407Z",
     "shell.execute_reply": "2025-04-15T17:05:48.059809Z",
     "shell.execute_reply.started": "2025-04-15T17:05:48.054914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_single_image(model, image_path, transform, num_images_per_sample, device):\n",
    "    \"\"\"\n",
    "    Fixed function to handle single image prediction with a multi-image model.\n",
    "    \"\"\"\n",
    "    # Load and transform the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    \n",
    "    # Create a batch with 2 copies of the same image to match expected shape [B, N, C, H, W]\n",
    "    # where N is num_images_per_sample (2 in your case)\n",
    "    image = torch.stack([image] * num_images_per_sample).unsqueeze(0).to(device)  # Shape: [1, N, C, H, W]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    # Access class names from the dataset\n",
    "    # class_names = [\"Mỡ\", \"Bào Ngư\", \"Đùi Gà\", \"Linh Chi Trắng\"]\n",
    "    # return class_names[pred]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4d28b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T15:21:41.810893Z",
     "iopub.status.idle": "2025-04-15T15:21:41.811090Z",
     "shell.execute_reply": "2025-04-15T15:21:41.811005Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.810997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# predict_single_image(model=model,\n",
    "#                      image_path=\"/kaggle/input/aio-hutech/test/001.jpg\",\n",
    "#                      transform=transform,\n",
    "#                      num_images_per_sample=num_images_per_sample,\n",
    "#                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453c215",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T15:21:41.811918Z",
     "iopub.status.idle": "2025-04-15T15:21:41.812151Z",
     "shell.execute_reply": "2025-04-15T15:21:41.812045Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.812030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def predict_folder(model, test_dir, transform, num_images_per_sample, device):\n",
    "#     for filename in os.listdir(test_dir):\n",
    "#         if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "#             path = os.path.join(test_dir, filename)\n",
    "#             predicted_class = predict_single_image(model, path, transform, num_images_per_sample, device)\n",
    "#             print(f\"{filename}: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc01a1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T15:21:41.813081Z",
     "iopub.status.idle": "2025-04-15T15:21:41.813954Z",
     "shell.execute_reply": "2025-04-15T15:21:41.813845Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.813830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# predict_folder(model,\n",
    "#                test_dir=\"/kaggle/input/aio-hutech/test\",\n",
    "#                transform=transform,\n",
    "#                num_images_per_sample=num_images_per_sample,\n",
    "#                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2c5a9f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:03:29.590717Z",
     "iopub.status.busy": "2025-04-15T16:03:29.590028Z",
     "iopub.status.idle": "2025-04-15T16:03:29.595702Z",
     "shell.execute_reply": "2025-04-15T16:03:29.594933Z",
     "shell.execute_reply.started": "2025-04-15T16:03:29.590693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict_folder_to_csv(model, test_dir, transform, num_images_per_sample, device, output_csv=\"predictions.csv\"):\n",
    "    results = []\n",
    "    for filename in os.listdir(test_dir):\n",
    "        if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            path = os.path.join(test_dir, filename)\n",
    "            predicted_class = predict_single_image(model, path, transform, num_images_per_sample, device)\n",
    "            file_id = os.path.splitext(filename)[0]  # Extract the prefix (ID) from the filename\n",
    "            results.append({'id': file_id, 'type': predicted_class})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Predictions saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cc5f4588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:15:03.133216Z",
     "iopub.status.busy": "2025-04-15T17:15:03.132989Z",
     "iopub.status.idle": "2025-04-15T17:15:09.385155Z",
     "shell.execute_reply": "2025-04-15T17:15:09.384395Z",
     "shell.execute_reply.started": "2025-04-15T17:15:03.133197Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /kaggle/working/submission-model4_10.csv\n"
     ]
    }
   ],
   "source": [
    "predict_folder_to_csv(model=model,\n",
    "                      test_dir=\"/kaggle/input/aio-hutech/test\",\n",
    "                      transform=transform,\n",
    "                      num_images_per_sample=num_images_per_sample,\n",
    "                      device=device,\n",
    "                      output_csv=\"/kaggle/working/submission-model4_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b7cfd62a-ce3c-4007-a419-f004ad360b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:24:09.964566Z",
     "iopub.status.busy": "2025-04-15T17:24:09.964257Z",
     "iopub.status.idle": "2025-04-15T17:24:09.972186Z",
     "shell.execute_reply": "2025-04-15T17:24:09.971524Z",
     "shell.execute_reply.started": "2025-04-15T17:24:09.964545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "\n",
    "def extract_num_images_from_model_name(model):\n",
    "    \"\"\"\n",
    "    Hàm phụ để trích xuất num_images_per_sample từ model.name (ví dụ: \"model_3\" → 3).\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d+)', getattr(model, \"name\", \"1\"))\n",
    "    return int(match.group(1)) if match else 1\n",
    "\n",
    "def ensemble_predict_single_image(models, image_path, transform, device, method=\"soft\"):\n",
    "    \"\"\"\n",
    "    Dự đoán nhãn của một ảnh bằng cách ensemble nhiều mô hình, tự động lấy num_images_per_sample từ tên model.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    prob_sum = None\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        num_images_per_sample = extract_num_images_from_model_name(model)\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = transform(image)\n",
    "        image_tensor = torch.stack([image_tensor] * num_images_per_sample).unsqueeze(0).to(device)  # [1, N, C, H, W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            pred = torch.argmax(probs, dim=1).item()\n",
    "            predictions.append(pred)\n",
    "\n",
    "            if prob_sum is None:\n",
    "                prob_sum = probs\n",
    "            else:\n",
    "                prob_sum += probs\n",
    "\n",
    "    if method == \"majority\":\n",
    "        return Counter(predictions).most_common(1)[0][0]\n",
    "    elif method == \"soft\":\n",
    "        avg_probs = prob_sum / len(models)\n",
    "        return torch.argmax(avg_probs, dim=1).item()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown ensemble method. Use 'soft' or 'majority'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a7ee3e82-6203-409f-9689-8b570e9e1d46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:24:14.396454Z",
     "iopub.status.busy": "2025-04-15T17:24:14.395814Z",
     "iopub.status.idle": "2025-04-15T17:24:14.401388Z",
     "shell.execute_reply": "2025-04-15T17:24:14.400643Z",
     "shell.execute_reply.started": "2025-04-15T17:24:14.396427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def ensemble_predict_folder_to_csv(models, test_dir, transform, device, method=\"soft\", output_csv=\"submission_ensemble.csv\"):\n",
    "    results = []\n",
    "    for filename in sorted(os.listdir(test_dir)):\n",
    "        if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            path = os.path.join(test_dir, filename)\n",
    "            predicted_class = ensemble_predict_single_image(\n",
    "                models=models,\n",
    "                image_path=path,\n",
    "                transform=transform,\n",
    "                device=device,\n",
    "                method=method\n",
    "            )\n",
    "            file_id = os.path.splitext(filename)[0]\n",
    "            results.append({'id': file_id, 'type': predicted_class})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Ensemble predictions saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "38badee0-acef-4d72-ac5f-70d773dae723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:25:07.294906Z",
     "iopub.status.busy": "2025-04-15T17:25:07.294643Z",
     "iopub.status.idle": "2025-04-15T17:25:13.657945Z",
     "shell.execute_reply": "2025-04-15T17:25:13.657350Z",
     "shell.execute_reply.started": "2025-04-15T17:25:07.294887Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_1 = ViT_MushroomClassifier(vit_model_name='vit_base_patch16_224', num_classes=num_classes).to(device)\n",
    "model_2 = ViT_MushroomClassifier(vit_model_name='vit_base_patch16_224', num_classes=num_classes).to(device)\n",
    "model_3 = ViT_MushroomClassifier(vit_model_name='vit_base_patch16_224', num_classes=num_classes).to(device)\n",
    "model_4 = ViT_MushroomClassifier(vit_model_name='vit_base_patch16_224', num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "908c2739-e1d9-4f20-9766-7eee6e3695b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:25:13.659329Z",
     "iopub.status.busy": "2025-04-15T17:25:13.659063Z",
     "iopub.status.idle": "2025-04-15T17:25:14.376561Z",
     "shell.execute_reply": "2025-04-15T17:25:14.375871Z",
     "shell.execute_reply.started": "2025-04-15T17:25:13.659306Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1176995031.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_1.load_state_dict(torch.load(\"vit_mushroom_multi_1_best.pth\"))\n",
      "/tmp/ipykernel_31/1176995031.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_2.load_state_dict(torch.load(\"vit_mushroom_multi_2_best.pth\"))\n",
      "/tmp/ipykernel_31/1176995031.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_3.load_state_dict(torch.load(\"vit_mushroom_multi_3_best.pth\"))\n",
      "/tmp/ipykernel_31/1176995031.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_4.load_state_dict(torch.load(\"vit_mushroom_multi_4_best.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.load_state_dict(torch.load(\"vit_mushroom_multi_1_best.pth\"))\n",
    "model_2.load_state_dict(torch.load(\"vit_mushroom_multi_2_best.pth\"))\n",
    "model_3.load_state_dict(torch.load(\"vit_mushroom_multi_3_best.pth\"))\n",
    "model_4.load_state_dict(torch.load(\"vit_mushroom_multi_4_best.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "13e08ffe-6a19-45ef-8ed7-8dcb69e508d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:27:49.930523Z",
     "iopub.status.busy": "2025-04-15T17:27:49.930216Z",
     "iopub.status.idle": "2025-04-15T17:28:00.952787Z",
     "shell.execute_reply": "2025-04-15T17:28:00.951971Z",
     "shell.execute_reply.started": "2025-04-15T17:27:49.930472Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble predictions saved to /kaggle/working/submission_ensemble_best_majority.csv\n"
     ]
    }
   ],
   "source": [
    "models = [model_1, model_2, model_3, model_4]  # các mô hình đã được load lên device\n",
    "\n",
    "ensemble_predict_folder_to_csv(\n",
    "    models=models,\n",
    "    test_dir=\"/kaggle/input/aio-hutech/test\",\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    method=\"majority\",  #\"soft\" hoặc \"majority\"\n",
    "    output_csv=\"/kaggle/working/submission_ensemble_best_majority.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c3169-8f4a-4076-af78-a7aec1a4ee4e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T15:21:41.822419Z",
     "iopub.status.idle": "2025-04-15T15:21:41.822658Z",
     "shell.execute_reply": "2025-04-15T15:21:41.822565Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.822556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def pseudo_label_test_set_group_by_class(model, test_folder, transform, device, num_images_per_sample=2, threshold=0.7):\n",
    "#     model.eval()\n",
    "#     image_paths = sorted(glob.glob(os.path.join(test_folder, \"*.jpg\")))\n",
    "#     pseudo_labels = []\n",
    "\n",
    "#     for path in image_paths:\n",
    "#         image = Image.open(path).convert(\"RGB\")\n",
    "#         image_tensor = transform(image).unsqueeze(0).to(device)  # [1, C, H, W]\n",
    "\n",
    "#         # Nhân bản ảnh này num_images_per_sample lần\n",
    "#         duplicated = image_tensor.repeat(num_images_per_sample, 1, 1, 1)  # [N, C, H, W]\n",
    "#         duplicated = duplicated.unsqueeze(0)  # [1, N, C, H, W]\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             output = model(duplicated)\n",
    "#             probs = F.softmax(output, dim=1)\n",
    "#             confidence, pred_class = torch.max(probs, dim=1)\n",
    "\n",
    "#         if confidence.item() >= threshold:\n",
    "#             pseudo_labels.append({\n",
    "#                 'image_paths': [path] * num_images_per_sample,  # nhân bản đúng theo format\n",
    "#                 'label': pred_class.item(),\n",
    "#                 'confidence': confidence.item()\n",
    "#             })\n",
    "\n",
    "#     return pseudo_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3b0a7-4705-4e5d-9a3e-81dbd71ea605",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T15:21:41.823734Z",
     "iopub.status.idle": "2025-04-15T15:21:41.824030Z",
     "shell.execute_reply": "2025-04-15T15:21:41.823891Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.823878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class MultiImagePseudoLabeledDataset(Dataset):\n",
    "#     def __init__(self, pseudo_labels, transform, num_images_per_sample=4):\n",
    "#         self.samples = pseudo_labels\n",
    "#         self.transform = transform\n",
    "#         self.num_images_per_sample = num_images_per_sample\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sample = self.samples[idx]\n",
    "#         imgs = []\n",
    "#         for path in sample['image_paths']:\n",
    "#             image = Image.open(path).convert(\"RGB\")\n",
    "#             if self.transform:\n",
    "#                 image = self.transform(image)\n",
    "#             imgs.append(image)\n",
    "#         return torch.stack(imgs), torch.tensor(sample['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0e462-c611-4b36-8c12-75e5e425fbb1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T15:21:41.825098Z",
     "iopub.status.idle": "2025-04-15T15:21:41.825378Z",
     "shell.execute_reply": "2025-04-15T15:21:41.825247Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.825238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pseudo_labels = pseudo_label_test_set_group_by_class(\n",
    "#     model=model,\n",
    "#     test_folder=\"/kaggle/input/aio-hutech/test\",\n",
    "#     transform=transform,\n",
    "#     device=device,\n",
    "#     num_images_per_sample=num_images_per_sample,\n",
    "#     threshold=0.7\n",
    "# )\n",
    "\n",
    "# pseudo_dataset = MultiImagePseudoLabeledDataset(\n",
    "#     pseudo_labels=pseudo_labels,\n",
    "#     transform=transform,\n",
    "#     num_images_per_sample=num_images_per_sample\n",
    "# )\n",
    "\n",
    "# pseudo_loader = DataLoader(pseudo_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e2641-a218-4ed5-8a99-becc9db27455",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T15:21:41.826316Z",
     "iopub.status.idle": "2025-04-15T15:21:41.826648Z",
     "shell.execute_reply": "2025-04-15T15:21:41.826496Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.826468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def fine_tune_with_pseudo_loader(model, pseudo_loader, device, epochs=5, lr=1e-4):\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         running_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         pbar = tqdm(pseudo_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "#         for images, labels in pbar:\n",
    "#             # images: [B, N, C, H, W]\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#             outputs = model(images)  # model đã nhận được [B, N, C, H, W]\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             _, preds = outputs.max(1)\n",
    "#             correct += (preds == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "\n",
    "#             pbar.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "#         epoch_loss = running_loss / len(pseudo_loader)\n",
    "#         epoch_acc = 100. * correct / total\n",
    "#         print(f\"✅ Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab745a2-8613-473f-a60b-362e199cdae9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T15:21:41.827744Z",
     "iopub.status.idle": "2025-04-15T15:21:41.827941Z",
     "shell.execute_reply": "2025-04-15T15:21:41.827855Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.827847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# finetuned_model = fine_tune_with_pseudo_loader(\n",
    "#     model=model,\n",
    "#     pseudo_loader=pseudo_loader,\n",
    "#     device=device,\n",
    "#     epochs=5,\n",
    "#     lr=1e-4\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c857fa-c940-4c06-8876-432e38ad5f3b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-15T15:21:41.828780Z",
     "iopub.status.idle": "2025-04-15T15:21:41.829045Z",
     "shell.execute_reply": "2025-04-15T15:21:41.828928Z",
     "shell.execute_reply.started": "2025-04-15T15:21:41.828914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Specify the path to the file you want to remove\n",
    "# file_path = \"/kaggle/working/submission-model_1.csv\"\n",
    "\n",
    "# # Check if the file exists, then remove it\n",
    "# if os.path.exists(file_path):\n",
    "#     os.remove(file_path)\n",
    "#     print(f\"File '{file_path}' has been removed.\")\n",
    "# else:\n",
    "#     print(f\"File '{file_path}' does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11378727,
     "sourceId": 95707,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
