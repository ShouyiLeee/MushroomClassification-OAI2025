{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c138ffb",
   "metadata": {},
   "source": [
    "# Sys check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ad0358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\IT\\\\GITHUB\\\\Hutech-AI-Challenge\\\\PatternFinding'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root = os.getcwd()\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd2949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 14 22:33:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   52C    P8              5W /   50W |     663MiB /   4096MiB |     25%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      7164    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A      8032    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9228    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9744    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     11732    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     11748    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12156    C+G   ... Access Service\\ePowerButton_NB.exe      N/A      |\n",
      "|    0   N/A  N/A     14216    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17552    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     23088    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     25848    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4a41a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df7dfc",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be37c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4e4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, optimizer, criterion, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {running_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        print(\"Validation performance:\")\n",
    "        evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26415dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean/std\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd2b454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 4\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-5\n",
    "num_images_per_sample=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a2ef3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm đánh giá\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"Abalone\", \"Baby Drumstick\", \"Button Mushroom\", \"White Lingzhi\"])\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69183753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class MultiImageMushroomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, num_images_per_sample=4):\n",
    "        \"\"\"\n",
    "        Dataset phân loại nấm sử dụng nhiều ảnh một mẫu.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Thư mục gốc chứa các class folders.\n",
    "            transform (callable, optional): Transform áp dụng lên ảnh.\n",
    "            num_images_per_sample (int): Số ảnh muốn nhóm lại thành 1 sample (mặc định 4).\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.num_images_per_sample = num_images_per_sample\n",
    "\n",
    "        # Map từ prefix sang class label\n",
    "        self.prefix2class = {\n",
    "            'NM': 'Button Mushroom',\n",
    "            'BN': 'Abalone',\n",
    "            'DG': 'Baby Drumstick',\n",
    "            'LC': 'White Lingzhi'\n",
    "        }\n",
    "        self.class_names = list(self.prefix2class.values())\n",
    "        self.class2idx = {name: idx for idx, name in enumerate(self.class_names)}\n",
    "\n",
    "        # Duyệt từng class folder\n",
    "        for class_name in self.class_names:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            images = sorted(glob.glob(os.path.join(class_path, \"*.jpg\")))\n",
    "\n",
    "            # Gom ảnh theo prefix (BM, AB, ...)\n",
    "            prefix_groups = {}\n",
    "            for img_path in images:\n",
    "                filename = os.path.basename(img_path)\n",
    "                match = re.match(r\"([A-Z]{2})\\d+\", filename)\n",
    "                if match:\n",
    "                    prefix = match.group(1)\n",
    "                    if prefix not in prefix_groups:\n",
    "                        prefix_groups[prefix] = []\n",
    "                    prefix_groups[prefix].append(img_path)\n",
    "\n",
    "            # Tạo samples từ các nhóm ảnh\n",
    "            for prefix, img_list in prefix_groups.items():\n",
    "                img_list = sorted(img_list)\n",
    "                label_idx = self.class2idx[self.prefix2class[prefix]]\n",
    "\n",
    "                for i in range(0, len(img_list), self.num_images_per_sample):\n",
    "                    selected = img_list[i:i+self.num_images_per_sample]\n",
    "                    if len(selected) < self.num_images_per_sample:\n",
    "                        selected = (selected + [selected[0]] * self.num_images_per_sample)[:self.num_images_per_sample]\n",
    "                    self.samples.append((selected, label_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_paths, label = self.samples[idx]\n",
    "        imgs = []\n",
    "\n",
    "        for path in img_paths:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            imgs.append(image)\n",
    "\n",
    "        return torch.stack(imgs), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "946e2f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from D:\\IT\\GITHUB\\Hutech-AI-Challenge\\data\\train...\n",
      "Dataset loaded: 300 samples\n",
      "Split: 240 training samples, 60 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm  # For progress bars in notebooks\n",
    "\n",
    "def create_dataloaders(dataset_root, transform, num_images_per_sample=2, \n",
    "                      batch_size=16, val_split=0.2, seed=42, \n",
    "                      num_workers=0, pin_memory=True):\n",
    "    \"\"\"\n",
    "    Create optimized train and validation dataloaders with better error handling\n",
    "    and performance settings.\n",
    "    \n",
    "    Args:\n",
    "        dataset_root (str): Root directory for dataset\n",
    "        transform: Data transformations to apply\n",
    "        num_images_per_sample (int): Number of images per sample\n",
    "        batch_size (int): Batch size for training\n",
    "        val_split (float): Validation split ratio (0-1)\n",
    "        seed (int): Random seed for reproducibility\n",
    "        num_workers (int): Number of workers for data loading (0 for no multiprocessing)\n",
    "        pin_memory (bool): Whether to pin memory for faster GPU transfer\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader)\n",
    "    \"\"\"\n",
    "    # Check if dataset directory exists\n",
    "    if not os.path.exists(dataset_root):\n",
    "        raise FileNotFoundError(f\"Dataset directory not found: {dataset_root}\")\n",
    "    \n",
    "    print(f\"Loading dataset from {dataset_root}...\")\n",
    "    \n",
    "    # Create dataset with progress reporting\n",
    "    try:\n",
    "        dataset = MultiImageMushroomDataset(\n",
    "            root_dir=dataset_root,\n",
    "            transform=transform,\n",
    "            num_images_per_sample=num_images_per_sample\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataset: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "    \n",
    "    # Set random seed for reproducible splits\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    val_size = int(len(dataset) * val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    \n",
    "    # Split dataset\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "    \n",
    "    print(f\"Split: {train_size} training samples, {val_size} validation samples\")\n",
    "    \n",
    "    # Create dataloaders with optimized settings\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,  # Set to 0 to avoid pickle errors\n",
    "        pin_memory=pin_memory,    # Faster data transfer to GPU\n",
    "        drop_last=False,          # Use all samples\n",
    "        persistent_workers=(num_workers > 0),  # Keep workers alive between epochs\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,  # No need to shuffle validation data\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Create dataloaders with optimized settings\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    dataset_root=\"D:\\\\IT\\\\GITHUB\\\\Hutech-AI-Challenge\\\\data\\\\train\",\n",
    "    transform=transform,\n",
    "    num_images_per_sample=num_images_per_sample,\n",
    "    batch_size=8,\n",
    "    num_workers=0,  # Fix pickle error by using 0 workers\n",
    "    pin_memory=torch.cuda.is_available(),  # Only pin if CUDA is available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2680d60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x27753b314f0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d0bb8bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label batch: tensor([1, 0, 2, 2, 3, 1, 1, 0])\n",
      "Max label: tensor(3)\n",
      "Min label: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(\"Label batch:\", labels)\n",
    "    print(\"Max label:\", labels.max())\n",
    "    print(\"Min label:\", labels.min())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "42ad3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViT_MushroomClassifier(nn.Module):\n",
    "    def __init__(self, vit_model_name='vit_base_patch16_224', num_classes=4):\n",
    "        super(ViT_MushroomClassifier, self).__init__()\n",
    "        self.vit = timm.create_model(vit_model_name, pretrained=True)\n",
    "        self.vit.head = nn.Identity()  # Bỏ classification head của ViT\n",
    "\n",
    "        self.embedding_dim = self.vit.num_features  # Thường là 768\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 4, C, H, W]\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B * N, C, H, W)\n",
    "\n",
    "        embeddings = self.vit(x)  # [B*4, D]\n",
    "        embeddings = embeddings.view(B, N, -1)  # [B, 4, D]\n",
    "\n",
    "        # Mean pooling over 4 embeddings\n",
    "        pooled = embeddings.mean(dim=1)  # [B, D]\n",
    "        out = self.classifier(pooled)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11172ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT_MushroomClassifier(vit_model_name='vit_base_patch16_224', num_classes=4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d9e550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss và optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e620afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training(model, optimizer, criterion, train_loader, val_loader, num_epochs=2, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952ba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        Abalone       1.00      1.00      1.00        32\n",
      " Baby Drumstick       1.00      1.00      1.00        31\n",
      "Button Mushroom       1.00      1.00      1.00        31\n",
      "  White Lingzhi       1.00      1.00      1.00        26\n",
      "\n",
      "       accuracy                           1.00       120\n",
      "      macro avg       1.00      1.00      1.00       120\n",
      "   weighted avg       1.00      1.00      1.00       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca33180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), \"vit_mushroom_multi.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"vit_liveness_multi.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(model, image_path, transform, num_images_per_sample, device):\n",
    "    \"\"\"\n",
    "    Fixed function to handle single image prediction with a multi-image model.\n",
    "    \"\"\"\n",
    "    # Load and transform the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    \n",
    "    # Create a batch with 2 copies of the same image to match expected shape [B, N, C, H, W]\n",
    "    # where N is num_images_per_sample (2 in your case)\n",
    "    image = torch.stack([image] * num_images_per_sample).unsqueeze(0).to(device)  # Shape: [1, N, C, H, W]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    # Access class names from the dataset\n",
    "    # class_names = [\"Mỡ\", \"Bào Ngư\", \"Đùi Gà\", \"Linh Chi Trắng\"]\n",
    "    # return class_names[pred]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4d28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_single_image(model=model,\n",
    "                     image_path=\"D:\\\\IT\\\\GITHUB\\\\Hutech-AI-Challenge\\\\data\\\\test\\\\165.jpg\",\n",
    "                     transform=transform,\n",
    "                     num_images_per_sample=num_images_per_sample,\n",
    "                     device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_folder(model, test_dir, transform, num_images_per_sample, device):\n",
    "    for filename in os.listdir(test_dir):\n",
    "        if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            path = os.path.join(test_dir, filename)\n",
    "            predicted_class = predict_single_image(model, path, transform, num_images_per_sample, device)\n",
    "            print(f\"{filename}: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc01a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001.jpg: 1\n",
      "002.jpg: 1\n",
      "003.jpg: 1\n",
      "004.jpg: 1\n",
      "005.jpg: 1\n",
      "006.jpg: 1\n",
      "007.jpg: 1\n",
      "008.jpg: 1\n",
      "009.jpg: 1\n",
      "010.jpg: 1\n",
      "011.jpg: 1\n",
      "012.jpg: 1\n",
      "013.jpg: 1\n",
      "014.jpg: 1\n",
      "015.jpg: 1\n",
      "016.jpg: 1\n",
      "017.jpg: 1\n",
      "018.jpg: 1\n",
      "019.jpg: 1\n",
      "020.jpg: 1\n",
      "021.jpg: 1\n",
      "022.jpg: 1\n",
      "023.jpg: 1\n",
      "024.jpg: 1\n",
      "025.jpg: 1\n",
      "026.jpg: 1\n",
      "027.jpg: 1\n",
      "028.jpg: 1\n",
      "029.jpg: 1\n",
      "030.jpg: 1\n",
      "031.jpg: 1\n",
      "032.jpg: 1\n",
      "033.jpg: 1\n",
      "034.jpg: 1\n",
      "035.jpg: 1\n",
      "036.jpg: 1\n",
      "037.jpg: 1\n",
      "038.jpg: 1\n",
      "039.jpg: 1\n",
      "040.jpg: 1\n",
      "041.jpg: 1\n",
      "042.jpg: 1\n",
      "043.jpg: 1\n",
      "044.jpg: 1\n",
      "045.jpg: 1\n",
      "046.jpg: 1\n",
      "047.jpg: 1\n",
      "048.jpg: 1\n",
      "049.jpg: 1\n",
      "050.jpg: 1\n",
      "051.jpg: 2\n",
      "052.jpg: 2\n",
      "053.jpg: 2\n",
      "054.jpg: 2\n",
      "055.jpg: 2\n",
      "056.jpg: 1\n",
      "057.jpg: 1\n",
      "058.jpg: 1\n",
      "059.jpg: 2\n",
      "060.jpg: 2\n",
      "061.jpg: 2\n",
      "062.jpg: 2\n",
      "063.jpg: 3\n",
      "064.jpg: 2\n",
      "065.jpg: 2\n",
      "066.jpg: 3\n",
      "067.jpg: 2\n",
      "068.jpg: 2\n",
      "069.jpg: 3\n",
      "070.jpg: 2\n",
      "071.jpg: 2\n",
      "072.jpg: 2\n",
      "073.jpg: 3\n",
      "074.jpg: 2\n",
      "075.jpg: 3\n",
      "076.jpg: 3\n",
      "077.jpg: 3\n",
      "078.jpg: 3\n",
      "079.jpg: 3\n",
      "080.jpg: 2\n",
      "081.jpg: 2\n",
      "082.jpg: 2\n",
      "083.jpg: 2\n",
      "084.jpg: 2\n",
      "085.jpg: 2\n",
      "086.jpg: 1\n",
      "087.jpg: 1\n",
      "088.jpg: 1\n",
      "089.jpg: 1\n",
      "090.jpg: 3\n",
      "091.jpg: 1\n",
      "092.jpg: 2\n",
      "093.jpg: 2\n",
      "094.jpg: 2\n",
      "095.jpg: 2\n",
      "096.jpg: 2\n",
      "097.jpg: 0\n",
      "098.jpg: 2\n",
      "099.jpg: 0\n",
      "100.jpg: 0\n",
      "101.jpg: 3\n",
      "102.jpg: 3\n",
      "103.jpg: 3\n",
      "104.jpg: 3\n",
      "105.jpg: 3\n",
      "106.jpg: 3\n",
      "107.jpg: 3\n",
      "108.jpg: 3\n",
      "109.jpg: 3\n",
      "110.jpg: 3\n",
      "111.jpg: 3\n",
      "112.jpg: 3\n",
      "113.jpg: 3\n",
      "114.jpg: 3\n",
      "115.jpg: 3\n",
      "116.jpg: 3\n",
      "117.jpg: 3\n",
      "118.jpg: 3\n",
      "119.jpg: 3\n",
      "120.jpg: 3\n",
      "121.jpg: 3\n",
      "122.jpg: 3\n",
      "123.jpg: 3\n",
      "124.jpg: 3\n",
      "125.jpg: 3\n",
      "126.jpg: 3\n",
      "127.jpg: 3\n",
      "128.jpg: 3\n",
      "129.jpg: 3\n",
      "130.jpg: 3\n",
      "131.jpg: 3\n",
      "132.jpg: 3\n",
      "133.jpg: 3\n",
      "134.jpg: 3\n",
      "135.jpg: 3\n",
      "136.jpg: 3\n",
      "137.jpg: 3\n",
      "138.jpg: 3\n",
      "139.jpg: 3\n",
      "140.jpg: 3\n",
      "141.jpg: 3\n",
      "142.jpg: 3\n",
      "143.jpg: 3\n",
      "144.jpg: 3\n",
      "145.jpg: 3\n",
      "146.jpg: 3\n",
      "147.jpg: 3\n",
      "148.jpg: 3\n",
      "149.jpg: 3\n",
      "150.jpg: 3\n",
      "151.jpg: 0\n",
      "152.jpg: 0\n",
      "153.jpg: 0\n",
      "154.jpg: 0\n",
      "155.jpg: 0\n",
      "156.jpg: 0\n",
      "157.jpg: 0\n",
      "158.jpg: 0\n",
      "159.jpg: 0\n",
      "160.jpg: 0\n",
      "161.jpg: 0\n",
      "162.jpg: 0\n",
      "163.jpg: 0\n",
      "164.jpg: 0\n",
      "165.jpg: 0\n",
      "166.jpg: 0\n",
      "167.jpg: 0\n",
      "168.jpg: 0\n",
      "169.jpg: 0\n",
      "170.jpg: 0\n",
      "171.jpg: 0\n",
      "172.jpg: 0\n",
      "173.jpg: 0\n",
      "174.jpg: 0\n",
      "175.jpg: 0\n",
      "176.jpg: 0\n",
      "177.jpg: 0\n",
      "178.jpg: 0\n",
      "179.jpg: 0\n",
      "180.jpg: 0\n",
      "181.jpg: 0\n",
      "182.jpg: 0\n",
      "183.jpg: 0\n",
      "184.jpg: 0\n",
      "185.jpg: 0\n",
      "186.jpg: 0\n",
      "187.jpg: 0\n",
      "188.jpg: 0\n",
      "189.jpg: 0\n",
      "190.jpg: 0\n",
      "191.jpg: 0\n",
      "192.jpg: 0\n",
      "193.jpg: 0\n",
      "194.jpg: 0\n",
      "195.jpg: 0\n",
      "196.jpg: 0\n",
      "197.jpg: 0\n",
      "198.jpg: 0\n",
      "199.jpg: 0\n",
      "200.jpg: 0\n"
     ]
    }
   ],
   "source": [
    "# model là mô hình ViT đã load từ checkpoint và đã .to(device)\n",
    "predict_folder(model,\n",
    "               test_dir=\"D:\\\\IT\\\\GITHUB\\\\Hutech-AI-Challenge\\\\data\\\\test\",\n",
    "               transform=transform,\n",
    "               num_images_per_sample=num_images_per_sample,\n",
    "               device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict_folder_to_csv(model, test_dir, transform, num_images_per_sample, device, output_csv=\"predictions.csv\"):\n",
    "    results = []\n",
    "    for filename in os.listdir(test_dir):\n",
    "        if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            path = os.path.join(test_dir, filename)\n",
    "            predicted_class = predict_single_image(model, path, transform, num_images_per_sample, device)\n",
    "            file_id = os.path.splitext(filename)[0]  # Extract the prefix (ID) from the filename\n",
    "            results.append({'id': file_id, 'type': predicted_class})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Predictions saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f4588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission-2.csv\n"
     ]
    }
   ],
   "source": [
    "predict_folder_to_csv(model=model,\n",
    "                      test_dir=\"D:\\\\IT\\\\GITHUB\\\\Hutech-AI-Challenge\\\\data\\\\test\",\n",
    "                      transform=transform,\n",
    "                      num_images_per_sample=num_images_per_sample,\n",
    "                      device=device,\n",
    "                      output_csv=\"submission-3.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Takehometest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
